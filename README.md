# Sign_language_classification
This project aims to develop a deep learning model that can accurately recognize sign language gestures. The ultimate goal of this project is to bridge communication gaps and empower individuals who are deaf or hard of hearing.
The model is trained on the Sign Language MNIST dataset, which consists of 27,455 grayscale images of sign language gestures, each of size 28x28 pixels. The model architecture consists of three convolutional layers, followed by two fully connected layers. The model is trained using data augmentation techniques to prevent overfitting. The final model achieves an accuracy of 99.5% on the test dataset. This project demonstrates the potential of deep learning to improve the lives of individuals with hearing impairments.

Dataset Link: https://www.kaggle.com/datasets/datamunge/sign-language-mnist/versions/1

